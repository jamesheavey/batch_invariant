# Dockerfile for vLLM with batch-invariant ops enabled
# Ensures deterministic outputs at temperature=0 with FlexAttention backend

FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.11 \
    python3.11-dev \
    python3.11-venv \
    python3-pip \
    git \
    build-essential \
    wget \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.11 as default
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1 && \
    update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1

WORKDIR /app

# Create and activate virtual environment
RUN python3 -m venv /app/venv
ENV PATH="/app/venv/bin:$PATH"

# Upgrade pip
RUN pip install --no-cache-dir --upgrade pip setuptools wheel

# Install PyTorch with CUDA support (2.5+ required for FlexAttention)
RUN pip install --no-cache-dir torch==2.5.1 --index-url https://download.pytorch.org/whl/cu121

# Install vLLM
RUN pip install --no-cache-dir vllm==0.7.3

# Copy batch-invariant ops package
COPY pyproject.toml /app/
COPY batch_invariant_ops/ /app/batch_invariant_ops/

# Install batch-invariant ops in editable mode
RUN pip install --no-cache-dir -e /app

# Copy sitecustomize.py to auto-enable batch-invariant mode in all workers
COPY vllm/sitecustomize.py /app/sitecustomize.py

# Set Python path to include sitecustomize.py
ENV PYTHONPATH="/app:${PYTHONPATH}"

# Environment variables for deterministic execution
ENV VLLM_ATTENTION_BACKEND=FLEX_ATTENTION
ENV CUBLAS_WORKSPACE_CONFIG=:16:8
ENV PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# Expose vLLM API port
EXPOSE 8000

# Default model (can be overridden at runtime)
ENV MODEL_NAME="meta-llama/Llama-3.1-8B-Instruct"
ENV MAX_MODEL_LEN=8192
ENV HOST=0.0.0.0
ENV PORT=8000

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=60s --retries=3 \
    CMD python3 -c "import requests; requests.get('http://localhost:8000/health')" || exit 1

# Start vLLM server with batch-invariant ops enabled
CMD vllm serve ${MODEL_NAME} \
    --host ${HOST} \
    --port ${PORT} \
    --max-model-len ${MAX_MODEL_LEN} \
    --dtype auto \
    --tokenizer-mode auto \
    --gpu-memory-utilization 0.9 \
    --enforce-eager

