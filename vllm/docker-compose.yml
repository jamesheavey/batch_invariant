version: "3.8"

services:
  vllm-deterministic:
    build:
      context: ..
      dockerfile: Dockerfile.vllm
    image: vllm-batch-invariant:latest
    container_name: vllm-deterministic
    runtime: nvidia
    environment:
      # Model configuration
      - MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct
      - MAX_MODEL_LEN=8192
      - HOST=0.0.0.0
      - PORT=8000

      # Deterministic execution
      - VLLM_ATTENTION_BACKEND=FLEX_ATTENTION
      - CUBLAS_WORKSPACE_CONFIG=:16:8

      # Optional: HuggingFace token for gated models
      # - HF_TOKEN=your_token_here

      # CUDA device selection (optional)
      - CUDA_VISIBLE_DEVICES=0

    ports:
      - "8000:8000"

    volumes:
      # Cache HuggingFace models to avoid re-downloading
      - ~/.cache/huggingface:/root/.cache/huggingface

    shm_size: "16gb"

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

    restart: unless-stopped

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s
